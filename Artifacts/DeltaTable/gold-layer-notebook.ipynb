{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dimension and Fact tables (Gold layer\n",
    "\n",
    "The gold layer is the final layer in the data pipeline, where the refined and transformed data resides for business intelligence and analytics purposes.\n",
    "1. The code creates a create 3 dataframes called `dim1`,`dim2`,`fact_cleansed` by reading the data from the `Dim1_cleansed` ,`Dim2_cleansed`,`Fact_cleansed` table. \n",
    "2. The code then creates a fact table by joining the `fact_cleansed` table with the `dim1` and `dim2` tables. It performs inner joins on the `key_column` between the these 3 tables/dataframes and selects specific columns from each table. It also computes a new column using expressions and transformations.\n",
    "4. The resulting joined data is selected and transformed, and the fact table is created with the specified table properties, comment, and configurations.\n",
    "\n",
    "Overall, the code creates dimension tables (`dim1` and `dim2`) and a fact table by joining the dimensions with the streaming data from `fact_cleansed`. The resulting tables are stored as Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr # used for data manipulation and transformation in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b9804a-a6db-43fd-b3c7-6cd7b90aa844",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dim1 = spark.read.table(\"Dim1_cleansed\")\n",
    "dim2 = spark.read.table(\"Dim2_cleansed\")\n",
    "fact_cleansed = spark.read.table(\"Fact_cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = fact_cleansed.join(dim1, fact_cleansed.key_column==dim1.key_column,\"inner\")\n",
    "                       .join(dim2, fact_cleansed.key_column==dim2.key_column,\"inner\") # # joins the three DataFrames based on the common column key_colum\n",
    "           # computes a new column using the select() function and various transformations such as col(), alias(), and expr().\n",
    "        .select(\n",
    "            \"fact_cleansed.column1\",\n",
    "            \"dim1.column2\",\n",
    "            \"dim2.column3\",\n",
    "            col(\"fact_cleansed.old_column_name\").cast(\"date\").alias(\"new_column_name\"),\n",
    "            \"fact_cleansed.column4\",\n",
    "            \"fact_cleansed.column5\",\n",
    "            expr(\"fact_cleansed.column4 * fact_cleansed.column5\").alias(\"new_column_name\"),\n",
    "        )\n",
    "##create a table from the DataFrame\n",
    "fact_df.write.format(\"delta\").saveAsTable(\"Fact\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RawFiles_gold-layer-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
