{"cells":[{"cell_type":"markdown","source":["## How to read schema from raw data files (Bronze layer)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3ffecd08-5108-4760-a797-d2b12fd30d27","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import * # used for data manipulation and transformation in Spark SQL\nfrom pyspark.sql.types import * # provides the data types that can be used to define the schema of a DataFrame or a column in Spark SQL\nimport datetime #imports the datetime module, which is a standard Python library for working with dates and times."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b90c5c48-a368-4ca5-a16e-2fae661e3430","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#create delta table DIm1\ndata_path1 ='/FileStore/tables/sample_emp_data.csv' # represents the file path where the data file is located ## to be changed to actual file name\ndf1 = spark.read.format(\"csv\")\\\n  .option(\"inferSchema\", \"false\")\\\n  .option(\"sep\",\",\")\\\n  .option(\"header\",\"true\")\\\n  .load(data_path1) ### read the data from the CSV file located at **data_path1** and load it into a DataFrame named **df1**\n##display(df1)\n\n## Now write out this DataFrame to a Delta table\ndf1.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Dim1_raw\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4fb5b929-b68c-40f3-9db3-1cbcd7a35940","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#create delta table Dim 2\ndata_path2 ='/FileStore/tables/sample_emp_data.csv' # represents the file path where the data file is located ## to be changed to actual file name\ndf2 = spark.read.format(\"csv\")\\\n  .option(\"inferSchema\", \"false\")\\\n  .option(\"sep\",\",\")\\\n  .option(\"header\",\"true\")\\\n  .load(data_path2) ### read the data from the CSV file located at **data_path2** and load it into a DataFrame named **df1**\n##display(df2)\n\n## Now write out this DataFrame to a Delta table\ndf2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Dim2_raw\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"365873b2-e632-4e73-8116-35553fb47848","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#create delta table Fact Table\ndata_path3 ='/FileStore/tables/sample_emp_data.csv' # represents the file path where the data file is located ## to be changed to actual file name\ndf3 = spark.read.format(\"csv\")\\\n  .option(\"inferSchema\", \"false\")\\\n  .option(\"sep\",\",\")\\\n  .option(\"header\",\"true\")\\\n  .load(data_path3) ### read the data from the CSV file located at **data_path3** and load it into a DataFrame named **df1**\n\n##display(df3)\n\n## Now write out this DataFrame to a Delta table\ndf3.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Fact_raw\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"76a1ad6d-798a-43c9-ab49-81e23f8686aa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"name":"python"},"application/vnd.databricks.v1+notebook":{"notebookName":"bronze-layer-notebook (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":547620698368321,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
