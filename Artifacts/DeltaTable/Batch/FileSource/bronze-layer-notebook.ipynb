{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ffecd08-5108-4760-a797-d2b12fd30d27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How to read schema from raw data files (Bronze layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90c5c48-a368-4ca5-a16e-2fae661e3430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * # used for data manipulation and transformation in Spark SQL\n",
    "from pyspark.sql.types import * # provides the data types that can be used to define the schema of a DataFrame or a column in Spark SQL\n",
    "import datetime #imports the datetime module, which is a standard Python library for working with dates and times."
   ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "service_credential = dbutils.secrets.get(scope='scope',key='service-credential-key')\n", 
        "spark.conf.set('fs.azure.account.auth.type.storage-account.dfs.core.windows.net', 'OAuth')\n",
        "spark.conf.set('fs.azure.account.oauth.provider.type.storage-account.dfs.core.windows.net'\n",
        "'org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider')\n",
        "spark.conf.set('fs.azure.account.oauth2.client.id.storage-account.dfs.core.windows.net', 'application-id')\n",
        "spark.conf.set('fs.azure.account.oauth2.client.secret.storage-account.dfs.core.windows.net', service_credential)\n",
        "spark.conf.set('fs.azure.account.oauth2.client.endpoint.storage-account.dfs.core.windows.net'\n", 
        "'https://login.microsoftonline.com/directory-id/oauth2/token')\n",
        "# Replace 'scope' with the Databricks secret scope name.\n",
        "# Replace 'service-credential-key' with the name of the key containing the client secret\n",
        "# Replace 'storage-account' with the name of the Azure storage account.\n",
        "# Replace 'application-id' with the Application (client) ID for the Azure Active Directory application.\n",
        "# Replace 'directory-id' with the Directory (tenant) ID for the Azure Active Directory application\n"
    ]
   },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb5b929-b68c-40f3-9db3-1cbcd7a35940",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create delta table DIm1\n",
    "data_path1 ='/FileStore/tables/sample_emp_data.csv' # represents the file path where the data file is located ## to be changed to actual file name\n",
    "df1 = spark.read.format(\"csv\")\\\n",
    "  .option(\"inferSchema\", \"false\")\\\n",
    "  .option(\"sep\",\",\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .load(data_path1) ### read the data from the CSV file located at **data_path1** and load it into a DataFrame named **df1**\n",
    "##display(df1)\n",
    "\n",
    "## Now write out this DataFrame to a Delta table\n",
    "df1.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Dim1_raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365873b2-e632-4e73-8116-35553fb47848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create delta table Dim 2\n",
    "data_path2 ='/FileStore/tables/sample_emp_data.csv' # represents the file path where the data file is located ## to be changed to actual file name\n",
    "df2 = spark.read.format(\"csv\")\\\n",
    "  .option(\"inferSchema\", \"false\")\\\n",
    "  .option(\"sep\",\",\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .load(data_path2) ### read the data from the CSV file located at **data_path2** and load it into a DataFrame named **df1**\n",
    "##display(df2)\n",
    "\n",
    "## Now write out this DataFrame to a Delta table\n",
    "df2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Dim2_raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a1ad6d-798a-43c9-ab49-81e23f8686aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create delta table Fact Table\n",
    "data_path3 ='/FileStore/tables/sample_emp_data.csv' # represents the file path where the data file is located ## to be changed to actual file name\n",
    "df3 = spark.read.format(\"csv\")\\\n",
    "  .option(\"inferSchema\", \"false\")\\\n",
    "  .option(\"sep\",\",\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .load(data_path3) ### read the data from the CSV file located at **data_path3** and load it into a DataFrame named **df1**\n",
    "\n",
    "##display(df3)\n",
    "\n",
    "## Now write out this DataFrame to a Delta table\n",
    "df3.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Fact_raw\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 547620698368321,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze-layer-notebook (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
