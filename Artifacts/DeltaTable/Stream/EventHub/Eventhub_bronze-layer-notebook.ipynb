{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer\n",
    "The bronze layer represents the raw, unprocessed data ingested into the data pipeline.\n",
    "The below code sets up streaming data ingestion from three Kafka topics and creates corresponding Delta tables for each topic. Here's a summary of the code:\n",
    "\n",
    "1. Configure parameters for connecting to Kafka topics and the Event Hub.\n",
    "2. Create the first bronze table (`dim1_raw`) by reading streaming data from Kafka topic 1 and converting the base64-encoded key and value columns to strings.\n",
    "3. Decorate the `dim1_raw` function with `@dlt.table` to create a Delta table with specified table properties and Spark configurations.\n",
    "4. Create the second bronze table (`dim2_raw`) by following similar steps as for `dim1_raw`, but reading from Kafka topic 2.\n",
    "5. Decorate the `dim2_raw` function with `@dlt.table` to create another Delta table.\n",
    "6. Create the third bronze table (`fact_raw`) by following similar steps as for `dim1_raw`, but reading from Kafka topic 3.\n",
    "7. Decorate the `fact_raw` function with `@dlt.table` to create the final Delta table.\n",
    "\n",
    "The code performs Kafka integration, and sets up Delta tables for further processing and analysis of the streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee820cb9-fd40-4cb1-8bd2-61d7323f5682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Configure parameters for connecting to Kafka topics and event hub\n",
    "\n",
    "Topic_Name1 = {topic1} # name of the first Kafka topic. \n",
    "Topic_Name2 = {topic2} # name of the second Kafka topic.\n",
    "Topic_Name3 = {topic3} # name of the third Kafka topic.\n",
    "EH_NS_NAME = {Event Hb Name}  # name of the event hub\n",
    "BOOTSTRAP_SERVERS = f\"{EH_NS_NAME}.servicebus.windows.net:9093\" # Bootstrap server address for connecting to Kafka\n",
    "SAKEY = \"UR+tdi5brOqFxphEl2rZdwszylRHA3tkwhOqsdqA464=\" # shared access key used for authentication with the event hub\n",
    "CONN_STRING = f\"Endpoint=sb://{EH_NS_NAME}.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey={SAKEY}\" # connection string for connecting to the event hub\n",
    "LOGIN_MODULE = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\" # login module used for authentication with Kafka\n",
    "EH_SASL = (\n",
    "    f'{LOGIN_MODULE} required username=\"$ConnectionString\" password=\"{CONN_STRING}\";' # SASL (Simple Authentication and Security Layer) configuration string for connecting to Kafka with the event hub\n",
    ")\n",
    "\n",
    "# --Create first bronze table--#\n",
    "\n",
    "# read streaming data from Kafka topic 1\n",
    "df1 = (\n",
    "    spark.readStream.format(\"kafka\") # Specifies format of the data source\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\") # Specifies bootstrap server of the Kafka cluster\n",
    "    .option(\"subscribe\", Topic_Name1) # Specifies topic(s) to subscribe to\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") # Specifies the SASL (Simple Authentication and Security Layer) mechanism to use for authentication with Kafka\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") # Specifies the security protocol to use for the connection. In this case, SASL_SSL is used, which combines SASL authentication with SSL encryption\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL) # Specifies the JAAS (Java Authentication and Authorization Service) configuration for SASL authentication\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\") # Specifies the timeout in milliseconds for Kafka requests.\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\") # Specifies the session timeout in milliseconds for Kafka consumer groups.\n",
    "    .option(\"failOnDataLoss\", \"false\") # Specifies whether to fail the stream query if data loss is detected. Setting it to \"false\" means that the query will continue processing even if data loss occurs.\n",
    "    .option(\"startingOffsets\", \"earliest\") # Specifies the starting offsets for reading from the Kafka topic. In this case, \"earliest\" is used to start reading from the earliest available offset.\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "df1 = df1.withColumn(\n",
    "    \"key\", col(\"key\").cast(\"string\")\n",
    ").withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "\n",
    "\n",
    "# Write into delta table (/data/delta/dim1_raw)\n",
    "##create a raw delta table from DataFrame\n",
    "df1.writeStream.format(\"delta\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"mergeSchema\", \"true\") \\\n",
    "   .option(\"checkpointLocation\", \"</data/delta/dim1_raw_checkpoint_path>\") \\ ##A checkpoint directory/location is required to track the streaming updates. If not specified , a default checkpoint directory is created at /local_disk0/tmp/.\n",
    "   .trigger(\"processing=30 seconds\") \\\n",
    "   .start(\"/data/delta/dim1_raw\")\n",
    "## .toTable(\"dim1_raw\")    ### Can be use .toTable instead of .start()\n",
    "\n",
    "\n",
    "#### --Create second bronze table--#####\n",
    "\n",
    "# read streaming data from Kafka topic 2\n",
    "df2 = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\") # Specifies bootstrap server of the Kafka cluster\n",
    "    .option(\"subscribe\", Topic_Name2)  # Specifies topic(s) to subscribe to\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") # Specifies the SASL (Simple Authentication and Security Layer) mechanism to use for authentication with Kafka\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") # Specifies the security protocol to use for the connection. In this case, SASL_SSL is used, which combines SASL authentication with SSL encryption \n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL) # Specifies the JAAS (Java Authentication and Authorization Service) configuration for SASL authentication\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\") # Specifies the timeout in milliseconds for Kafka requests.\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\") # Specifies the session timeout in milliseconds for Kafka consumer groups\n",
    "    .option(\"failOnDataLoss\", \"false\") # Specifies whether to fail the stream query if data loss is detected. Setting it to \"false\" means that the query will continue processing even if data loss occurs.\n",
    "    .option(\"startingOffsets\", \"earliest\") # Specifies the starting offsets for reading from the Kafka topic. In this case, \"earliest\" is used to start reading from the earliest available offset.\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "df2 = df2.withColumn(\"key\", col(\"key\").cast(\"string\")).withColumn(\n",
    "    \"value\", col(\"value\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Write into delta table (/data/delta/dim2_raw)\n",
    "##create a raw delta table from DataFrame\n",
    "df2.writeStream.format(\"delta\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"mergeSchema\", \"true\") \\\n",
    "   .option(\"checkpointLocation\", \"</data/delta/dim2_raw_checkpoint_path>\") \\ ##A checkpoint directory/location is required to track the streaming updates. If not specified , a default checkpoint directory is created at /local_disk0/tmp/.\n",
    "   .trigger(\"processing=30 seconds\") \\\n",
    "   .start(\"/data/delta/dim2_raw\")\n",
    "## .toTable(\"dim2_raw\")    ### Can be use .toTable instead of .start()\n",
    "\n",
    "\n",
    "#### --Create third bronze table--####\n",
    "\n",
    "# read streaming data from Kafka topic 3\n",
    "df3 = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\") # Specifies bootstrap server of the Kafka cluster\n",
    "    .option(\"subscribe\", Topic_Name3)  # Specifies topic(s) to subscribe to\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") # Specifies the SASL (Simple Authentication and Security Layer) mechanism to use for authentication with Kafka\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") # Specifies the security protocol to use for the connection. In this case, SASL_SSL is used, which combines SASL authentication with SSL encryption\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL) # Specifies the JAAS (Java Authentication and Authorization Service) configuration for SASL authentication\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\") # Specifies the timeout in milliseconds for Kafka requests.\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\") # Specifies the session timeout in milliseconds for Kafka consumer groups\n",
    "    .option(\"failOnDataLoss\", \"false\") # Specifies whether to fail the stream query if data loss is detected. Setting it to \"false\" means that the query will continue processing even if data loss occurs.\n",
    "    .option(\"startingOffsets\", \"earliest\") # Specifies the starting offsets for reading from the Kafka topic. In this case, \"earliest\" is used to start reading from the earliest available offset.\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "df3 = df3.withColumn(\"key\", col(\"key\").cast(\"string\")).withColumn(\n",
    "    \"value\", col(\"value\").cast(\"string\")\n",
    ")\n",
    "\n",
    "\n",
    "# Write into delta table (/data/delta/dim3_raw)\n",
    "##create a raw delta table from DataFrame\n",
    "df3.writeStream.format(\"delta\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"mergeSchema\", \"true\") \\\n",
    "   .option(\"checkpointLocation\", \"</data/delta/dim3_raw_checkpoint_path>\") \\ ##A checkpoint directory/location is required to track the streaming updates. If not specified , a default checkpoint directory is created at /local_disk0/tmp/.\n",
    "   .trigger(\"processing=30 seconds\") \\\n",
    "   .start(\"/data/delta/dim3_raw\")\n",
    "## .toTable(\"dim2_raw\")    ### Can be use .toTable instead of .start()\n",
    "\n",
    "#################End of File ##############"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Eventhub_bronze-layer-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
