{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer\n",
    "The gold layer is the final layer in the data pipeline, where the refined and transformed data resides for business intelligence and analytics purposes.\n",
    "1. The code creates a Delta table called `dim1` by reading the streaming data from the `dim1_cleansed` table. It adds an identity column for surrogate key and defines the table properties, schema, and other configurations.\n",
    "2. Similarly, the code creates another Delta table called `dim2` by reading the streaming data from the `dim2_cleansed` table. It also adds an identity column for surrogate key and specifies the table properties, schema, and configurations.\n",
    "3. The code then creates a fact table by joining the `fact_cleansed` table with the `dim1` and `dim2` tables. It performs inner joins on the `key_column` between the streams and selects specific columns from each table. It also computes a new column using expressions and transformations.\n",
    "4. The resulting joined data is selected and transformed, and the fact table is created with the specified table properties, comment, and configurations.\n",
    "\n",
    "Overall, the code creates dimension tables (`dim1` and `dim2`) and a fact table by joining the dimensions with the streaming data from `fact_cleansed`. The resulting tables are stored as Delta tables with the specified properties and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f343a1a4-26b9-49b7-b019-1e4ad4283d4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema={gold_layer_schema}, # specifies the schema of the table\n",
    "\n",
    "# read data from streaming source dim1_cleansed\n",
    "dim1 = spark.readStream.format(\"delta\")\\\n",
    "  .option(\"maxFilesPerTrigger\",5)\\ # maxFilesPerTrigger specifies the maximum number of new files to be considered in every trigger,default value is 1000\n",
    "  .option(\"ignoreChanges\",\"true\")\\\n",
    "  .load(\"{Delta table path for dim1_cleansed}\")\n",
    "\n",
    "# read data from streaming source dim2_cleansed\n",
    "dim2 = spark.readStream.format(\"delta\")\\\n",
    "  .option(\"maxFilesPerTrigger\",5)\\ # maxFilesPerTrigger specifies the maximum number of new files to be considered in every trigger,default value is 1000\n",
    "  .option(\"ignoreChanges\",\"true\")\\\n",
    "  .load(\"{Delta table path for dim2_cleansed}\")\n",
    "\n",
    "# read data from streaming source fact_cleansed\n",
    "fact_cleansed = spark.readStream.format(\"delta\")\\\n",
    "  .option(\"maxFilesPerTrigger\",5)\\ # maxFilesPerTrigger specifies the maximum number of new files to be considered in every trigger,default value is 1000\n",
    "  .option(\"ignoreChanges\",\"true\")\\\n",
    "  .load(\"{Delta table path for fact_cleansed}\")\n",
    "\n",
    "########Creating fact_data frame after the joins and transforamtion##########\n",
    "\n",
    "fact_df = fact_cleansed.join(dim1, fact_cleansed.key_column==dim1.key_column,\"inner\")\n",
    "                       .join(dim2, fact_cleansed.key_column==dim2.key_column,\"inner\") \n",
    "           # computes a new column using the select() function and various transformations such as col(), alias(), and expr().\n",
    "        .select(\n",
    "            \"fact_cleansed.column1\",\n",
    "            \"dim1.column2\",\n",
    "            \"dim2.column3\",\n",
    "            col(\"fact_cleansed.old_column_name\").cast(\"date\").alias(\"new_column_name\"),\n",
    "            \"fact_cleansed.column4\",\n",
    "            \"fact_cleansed.column5\",\n",
    "            expr(\"fact_cleansed.column4 * fact_cleansed.column5\").alias(\"new_column_name\"),\n",
    "        )\n",
    "    \n",
    "# Write into delta table (/data/delta/fact) ,creating a silver delta table from fact_df DataFrame\n",
    "fact_df.writeStream.format(\"delta\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"mergeSchema\", \"true\") \\\n",
    "   .trigger(\"processing=30 seconds\") \\\n",
    "   .option(\"checkpointLocation\", \"</data/delta/fact_checkpoint_path>\") \\ ##A checkpoint directory/location is required to track the streaming updates. If not specified , a default checkpoint directory is created at /local_disk0/tmp/.\n",
    "   .table(\"fact\")\n",
    "\n",
    "###########End of File ##################"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EventHub_gold-layer-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
