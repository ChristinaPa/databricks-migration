{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer\n",
    "The gold layer is the final layer in the data pipeline, where the refined and transformed data resides for business intelligence and analytics purposes.\n",
    "1. The code creates a Delta table called `dim1` by reading the streaming data from the `dim1_cleansed` table. It adds an identity column for surrogate key and defines the table properties, schema, and other configurations.\n",
    "2. Similarly, the code creates another Delta table called `dim2` by reading the streaming data from the `dim2_cleansed` table. It also adds an identity column for surrogate key and specifies the table properties, schema, and configurations.\n",
    "3. The code then creates a fact table by joining the `fact_cleansed` table with the `dim1` and `dim2` tables. It performs inner joins on the `key_column` between the streams and selects specific columns from each table. It also computes a new column using expressions and transformations.\n",
    "4. The resulting joined data is selected and transformed, and the fact table is created with the specified table properties, comment, and configurations.\n",
    "\n",
    "Overall, the code creates dimension tables (`dim1` and `dim2`) and a fact table by joining the dimensions with the streaming data from `fact_cleansed`. The resulting tables are stored as Delta tables with the specified properties and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f343a1a4-26b9-49b7-b019-1e4ad4283d4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dimension1 table from Delta Live Table dim1_cleansed by adding identity column for surrogate key \n",
    "@dlt.table(\n",
    "    schema={gold_layer_schema}, # specifies the schema of the table\n",
    "    comment=\"Load data to dimension1 table\",\n",
    "    table_properties={\"quality\": \"gold\", \"pipelines.reset.allowed\": \"true\"}, # indicates the quality level of the table and pipeline resets for the table respectively\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"}, # trigger interval for the table's pipeline\n",
    "    temporary=False, # indicates that the table is persistent\n",
    ")\n",
    "def dim1():\n",
    "    return dlt.read_stream(\"dim1_cleansed\")\n",
    "\n",
    "# Create dimension2 table from Delta Live Table dim2_cleansed by adding identity column for surrogate key \n",
    "@dlt.table(\n",
    "    schema={gold_layer_schema}, # specifies the schema of the table\n",
    "    comment=\"Load data to dimension2 table\",\n",
    "    table_properties={\"quality\": \"gold\", \"pipelines.reset.allowed\": \"true\"}, # indicates the quality level of the table and pipeline resets for the table respectively\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"}, # trigger interval for the table's pipeline\n",
    "    temporary=False, # indicates that the table is persistent\n",
    ")\n",
    "def dim2():\n",
    "    return dlt.read_stream(\"dim2_cleansed\")\n",
    "\n",
    "# Create Fact Table by joining dimension tables with the third table\n",
    "@dlt.table(\n",
    "    comment=\"load data to fact table\",\n",
    "    table_properties={\"quality\": \"gold\", \"pipelines.reset.allowed\": \"true\"}, # indicates the quality level of the table and pipeline resets for the table respectively\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"}, # trigger interval for the table's pipeline\n",
    "    temporary=False, # indicates that the table is persistent\n",
    ")\n",
    "def fact():\n",
    "    s = dlt.read_stream(\"fact_cleansed\").alias(\"s\")\n",
    "    p = dlt.read_stream(\"dim1\").alias(\"p\")\n",
    "    c = dlt.read_stream(\"dim2\").alias(\"c\")\n",
    "    return ( \n",
    "        # inner joins on the key_column between the streams s (representing fact_cleansed), p (representing dim1), and c (representing dim2)\n",
    "        s.join(p, s.key_column == p.key_column, \"inner\")\n",
    "        .join(c, s.key_column == c.key_column, \"inner\")\n",
    "        # computes a new column using the select() function and various transformations such as col(), alias(), and expr().\n",
    "        .select(\n",
    "            \"s.column1\",\n",
    "            \"c.column2\",\n",
    "            \"p.column3\",\n",
    "            col(\"s.old_column_name\").cast(\"date\").alias(\"new_column_name\"),\n",
    "            \"s.column4\",\n",
    "            \"s.column5\",\n",
    "            expr(\"s.column4 * s.column5\").alias(\"new_column_name\"),\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EventHub_gold-layer-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
