{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f5bb433-7586-428c-96cb-730e2b60aeab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "EH_NS_NAME = \"az-cslabs-event-hub-ns\"\n",
    "BOOTSTRAP_SERVERS = f\"{EH_NS_NAME}.servicebus.windows.net:9093\"\n",
    "SAKEY = \"UR+tdi5brOqFxphEl2rZdwszylRHA3tkwhOqsdqA464=\"\n",
    "CONN_STRING = f\"Endpoint=sb://{EH_NS_NAME}.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey={SAKEY}\"\n",
    "LOGIN_MODULE = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n",
    "EH_SASL = (\n",
    "    f'{LOGIN_MODULE} required username=\"$ConnectionString\" password=\"{CONN_STRING}\";'\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --Create Sales orders table--#\n",
    "\n",
    "sales_orders_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\")\n",
    "    .option(\"subscribe\", \"retail.public.sales_orders\")  # Saled orders topic\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL)\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "sales_orders_df = sales_orders_df.withColumn(\n",
    "    \"key\", col(\"key\").cast(\"string\")\n",
    ").withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "\n",
    "# sales_orders_df.display()\n",
    "\n",
    "# Create raw delta live table\n",
    "@dlt.table(\n",
    "    table_properties={\"pipelines.reset.allowed\": \"false\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"1 seconds\"},\n",
    ")\n",
    "def sales_orders_raw():\n",
    "    return sales_orders_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --Create Customers table--#\n",
    "customers_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\")\n",
    "    .option(\"subscribe\", \"retail.public.customers\")  # Customers topic\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL)\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "customers_df = customers_df.withColumn(\"key\", col(\"key\").cast(\"string\")).withColumn(\n",
    "    \"value\", col(\"value\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# df.display()\n",
    "\n",
    "# Create raw delta live table\n",
    "@dlt.table(\n",
    "    #     table_properties={\"pipelines.reset.allowed\": \"false\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"1 seconds\"},\n",
    ")\n",
    "def customers_raw():\n",
    "    return customers_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --Create Products table--#\n",
    "products_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\")\n",
    "    .option(\"subscribe\", \"retail.public.products\")  # Products topic\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL)\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "products_df = products_df.withColumn(\"key\", col(\"key\").cast(\"string\")).withColumn(\n",
    "    \"value\", col(\"value\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# df.display()\n",
    "\n",
    "# Create raw delta live table\n",
    "@dlt.table(\n",
    "    table_properties={\"pipelines.reset.allowed\": \"false\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"1 seconds\"},\n",
    ")\n",
    "def products_raw():\n",
    "    return products_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "sales_orders_schema = StructType(\n",
    "    [\n",
    "        StructField(\"customer_id\", LongType(), True),\n",
    "        StructField(\"customer_name\", StringType(), True),\n",
    "        StructField(\"order_datetime\", StringType(), True),\n",
    "        StructField(\"order_number\", LongType(), True),\n",
    "        StructField(\n",
    "            \"ordered_products\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"curr\", StringType(), True),\n",
    "                        StructField(\"id\", StringType(), True),\n",
    "                        StructField(\"name\", StringType(), True),\n",
    "                        StructField(\"price\", IntegerType(), True),\n",
    "                        StructField(\"qty\", IntegerType(), True),\n",
    "                        StructField(\"unit\", StringType(), True),\n",
    "                        StructField(\n",
    "                            \"promotion_info\",\n",
    "                            StructType(\n",
    "                                [\n",
    "                                    StructField(\"promo_id\", IntegerType(), True),\n",
    "                                    StructField(\"promo_qty\", IntegerType(), True),\n",
    "                                    StructField(\"promo_disc\", DecimalType(3, 2), True),\n",
    "                                    StructField(\"promo_item\", StringType(), True),\n",
    "                                ]\n",
    "                            ),\n",
    "                            True,\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "        StructField(\"number_of_line_items\", LongType(), True),\n",
    "        StructField(\n",
    "            \"clicked_items\", ArrayType(ArrayType(StringType(), True), True), True\n",
    "        ),\n",
    "        StructField(\n",
    "            \"promo_info\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"promo_id\", IntegerType(), True),\n",
    "                        StructField(\"promo_qty\", IntegerType(), True),\n",
    "                        StructField(\"promo_disc\", DecimalType(3, 2), True),\n",
    "                        StructField(\"promo_item\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Load data to sales_orders cleansed table\",\n",
    "    table_properties={\"pipelines.reset.allowed\": \"true\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"},\n",
    "    temporary=False,\n",
    ")\n",
    "def sales_orders_cleansed():\n",
    "    return (\n",
    "        dlt.read_stream(\"sales_orders_raw\")\n",
    "        #         spark.readStream.format(\"delta\").table(\"retail_org.sales_orders_raw\")\n",
    "        .select(get_json_object(col(\"value\"), \"$.payload.after\").alias(\"row\"))\n",
    "        .withColumn(\"row\", regexp_replace(\"row\", '\"\\\\[', \"[\"))\n",
    "        .withColumn(\"row\", regexp_replace(\"row\", '\\\\]\"', \"]\"))\n",
    "        .withColumn(\"row\", regexp_replace(\"row\", \"\\\\\\\\\", \"\"))\n",
    "        .select(from_json(col(\"row\"), sales_orders_schema).alias(\"row\"))\n",
    "        .select(\"row.*\")\n",
    "        .withColumn(\"ordered_products\", explode(\"ordered_products\"))\n",
    "        .withColumn(\"order_datetime\", from_unixtime(\"order_datetime\"))\n",
    "        .withColumn(\"product_id\", col(\"ordered_products\").id)\n",
    "        .withColumn(\"unit_price\", col(\"ordered_products\").price)\n",
    "        .withColumn(\"quantity\", col(\"ordered_products\").qty)\n",
    "    )\n",
    "\n",
    "\n",
    "# sales_orders_cleansed().display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "customers_schema = StructType(\n",
    "    [\n",
    "        StructField(\"customer_id\", LongType(), False),\n",
    "        StructField(\"tax_id\", StringType(), True),\n",
    "        StructField(\"tax_code\", StringType(), True),\n",
    "        StructField(\"customer_name\", StringType(), False),\n",
    "        StructField(\"state\", StringType(), False),\n",
    "        StructField(\"city\", StringType(), False),\n",
    "        StructField(\"postcode\", StringType(), False),\n",
    "        StructField(\"street\", StringType(), False),\n",
    "        StructField(\"number\", StringType(), False),\n",
    "        StructField(\"unit\", StringType(), False),\n",
    "        StructField(\"region\", StringType(), False),\n",
    "        StructField(\"district\", StringType(), False),\n",
    "        StructField(\"lon\", DecimalType(10, 6), False),\n",
    "        StructField(\"lat\", DecimalType(10, 6), False),\n",
    "        StructField(\"ship_to_address\", StringType(), False),\n",
    "        StructField(\n",
    "            \"valid_from\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"scale\", IntegerType(), False),\n",
    "                    StructField(\"value\", StringType(), False),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        StructField(\n",
    "            \"valid_to\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"scale\", IntegerType(), False),\n",
    "                    StructField(\"value\", StringType(), False),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        StructField(\n",
    "            \"units_purchased\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"scale\", IntegerType(), False),\n",
    "                    StructField(\"value\", StringType(), False),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        StructField(\"loyalty_segment\", StringType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Load data to customers cleansed table\",\n",
    "    table_properties={\"pipelines.reset.allowed\": \"true\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"},\n",
    "    temporary=False,\n",
    ")\n",
    "def customers_cleansed():\n",
    "    return (\n",
    "        dlt.read_stream(\"customers_raw\")\n",
    "        #         spark.readStream.format(\"delta\").table(\"retail_org.customers_raw\")\n",
    "        #          spark.read.format(\"delta\").table(\"retail_org.customers_raw\")\n",
    "        .select(get_json_object(col(\"value\"), \"$.payload.after\").alias(\"row\"))\n",
    "        .select(from_json(col(\"row\"), customers_schema).alias(\"row\"))\n",
    "        .select(\"row.*\")\n",
    "    )\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "products_schema = StructType(\n",
    "    [\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"product_category\", StringType(), False),\n",
    "        StructField(\"product_name\", StringType(), False),\n",
    "        StructField(\n",
    "            \"sales_price\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"scale\", IntegerType(), False),\n",
    "                    StructField(\"value\", StringType(), False),\n",
    "                ]\n",
    "            ),\n",
    "            False,\n",
    "        ),\n",
    "        StructField(\"ean13\", DoubleType(), False),\n",
    "        StructField(\"ean5\", StringType(), False),\n",
    "        StructField(\"product_unit\", StringType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Load data to a products cleansed table\",\n",
    "    table_properties={\"pipelines.reset.allowed\": \"true\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"},\n",
    "    temporary=False,\n",
    ")\n",
    "def products_cleansed():\n",
    "    return (\n",
    "        dlt.read_stream(\"products_raw\")\n",
    "        #         spark.readStream.format(\"delta\").table(\"retail_org.products_raw\")\n",
    "        #         spark.read.format(\"delta\").table(\"retail_org.products_raw\")\n",
    "        .select(get_json_object(col(\"value\"), \"$.payload.after\").alias(\"row\"))\n",
    "        .select(from_json(col(\"row\"), products_schema).alias(\"row\"))\n",
    "        .select(\"row.*\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create products dimension table by adding identity column for surrogate key \n",
    "@dlt.table(\n",
    "    schema=\"\"\"\n",
    "         product_key BIGINT GENERATED ALWAYS AS identity,\n",
    "         product_id STRING,\n",
    "         product_category STRING,\n",
    "         product_name STRING,\n",
    "         sales_price STRUCT<scale: INT, value: STRING>,\n",
    "         ean13 DOUBLE,\n",
    "         ean5 STRING,\n",
    "         product_unit STRING    \n",
    "    \"\"\",\n",
    "    comment=\"Load data to products dimension table\",\n",
    "    table_properties={\"quality\": \"gold\", \"pipelines.reset.allowed\": \"true\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"},\n",
    "    temporary=False,\n",
    ")\n",
    "def dim_products():\n",
    "    return dlt.read_stream(\"products_cleansed\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create customers dimension table by adding identity column for surrogate key \n",
    "@dlt.table(\n",
    "    schema=\"\"\"\n",
    "          customer_key BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "          customer_id BIGINT,\n",
    "          tax_id STRING,\n",
    "          tax_code STRING,\n",
    "          customer_name STRING,\n",
    "          state STRING,\n",
    "          city STRING,\n",
    "          postcode STRING,\n",
    "          street STRING,\n",
    "          number STRING,\n",
    "          unit STRING,\n",
    "          region STRING,\n",
    "          district STRING,\n",
    "          lon DECIMAL(10,6),\n",
    "          lat DECIMAL(10,6),\n",
    "          ship_to_address STRING,\n",
    "          valid_from STRUCT<scale: INT, value: STRING>,\n",
    "          valid_to STRUCT<scale: INT, value: STRING>,\n",
    "          units_purchased STRUCT<scale: INT, value: STRING>,\n",
    "          loyalty_segment STRING\n",
    "    \"\"\",\n",
    "    comment=\"Load data to customers dimension table\",\n",
    "    table_properties={\"quality\": \"gold\", \"pipelines.reset.allowed\": \"true\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"},\n",
    "    temporary=False,\n",
    ")\n",
    "def dim_customers():\n",
    "    return dlt.read_stream(\"customers_cleansed\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"load data to sales orders fact table\",\n",
    "    table_properties={\"quality\": \"gold\", \"pipelines.reset.allowed\": \"true\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"},\n",
    "    temporary=False,\n",
    ")\n",
    "def fact_sales_orders():\n",
    "    s = dlt.read_stream(\"sales_orders_cleansed\").alias(\"s\")\n",
    "    p = dlt.read_stream(\"dim_products\").alias(\"p\")\n",
    "    c = dlt.read_stream(\"dim_customers\").alias(\"c\")\n",
    "    return (\n",
    "        s.join(p, s.product_id == p.product_id, \"inner\")\n",
    "        .join(c, s.customer_id == c.customer_id, \"inner\")\n",
    "        .select(\n",
    "            \"s.order_number\",\n",
    "            \"c.customer_key\",\n",
    "            \"p.product_key\",\n",
    "            col(\"s.order_datetime\").cast(\"date\").alias(\"order_date\"),\n",
    "            \"s.unit_price\",\n",
    "            \"s.quantity\",\n",
    "            expr(\"s.unit_price * s.quantity\").alias(\"total_price\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"load data to customer sales fact table\",\n",
    "    table_properties={\"quality\": \"gold\", \"pipelines.reset.allowed\": \"true\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"60 seconds\"},\n",
    "    temporary=False,\n",
    ")\n",
    "def fact_customer_sales():\n",
    "    s = dlt.read(\"sales_orders_cleansed\").alias(\"s\")\n",
    "    p = dlt.read(\"dim_products\").alias(\"p\")\n",
    "    c = dlt.read(\"dim_customers\").alias(\"c\")\n",
    "    return (\n",
    "        s.join(p, s.product_id == p.product_id, \"inner\")\n",
    "        .join(c, s.customer_id == c.customer_id, \"inner\")\n",
    "        .groupBy(\"c.customer_key\", \"p.product_key\")\n",
    "        .agg(\n",
    "            sum(\"quantity\").alias(\"total_quantity\"),\n",
    "            sum(expr(\"s.unit_price * s.quantity\")).alias(\"sale_amount\"),\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EventHub_example",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
