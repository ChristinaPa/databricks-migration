{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d2e204-e787-42a9-acd1-2310d72f2250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8439f5c2-f9c7-4be0-b0cd-50b26e487cea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=\"databricksscopename\",key=\"databricksservicekey\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"5c962712-e111-48b0-b9af-41f03175e216\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adlsx5u224gv3xgd2.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91af8b2e-4ad9-4d9c-a742-d22575ae1f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = ArrayType(\n",
    "    StructType(\n",
    "        [\n",
    "            StructField(\"qty\", IntegerType(), True),\n",
    "            StructField(\"unit\", StringType(), True),\n",
    "            StructField(\"curr\", StringType(), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"price\", IntegerType(), True),\n",
    "            StructField(\n",
    "                \"promotion_info\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"promo_id\", IntegerType(), True),\n",
    "                        StructField(\"promo_qty\", IntegerType(), True),\n",
    "                        StructField(\"promo_disc\", DecimalType(3, 2), True),\n",
    "                        StructField(\"promo_item\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "schema2 = ArrayType(\n",
    "    StructType(\n",
    "        [\n",
    "            StructField(\"promo_id\", IntegerType(), True),\n",
    "            StructField(\"promo_qty\", IntegerType(), True),\n",
    "            StructField(\"promo_disc\", DecimalType(3, 2), True),\n",
    "            StructField(\"promo_item\", StringType(), True),\n",
    "        ]\n",
    "    ),\n",
    "    True,\n",
    ")\n",
    "schema3 = ArrayType(ArrayType(StringType(), True), True)\n",
    "\n",
    "df1 = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"dbfs:/user/hive/warehouse/retail_org_dt.db/sales_orders_raw\") \\\n",
    "        .select(\"*\") \\\n",
    "        .withColumn(\"clicked_items\", from_json(col(\"clicked_items\"), schema3)) \\\n",
    "        .withColumn(\"promo_info\", from_json(col(\"promo_info\"), schema2)) \\\n",
    "        .withColumn(\"ordered_products\", from_json(col(\"ordered_products\"), schema)) \\\n",
    "        .withColumn(\"ordered_products\", explode(\"ordered_products\")) \\\n",
    "        .withColumn(\"order_datetime\", from_unixtime(\"order_datetime\")) \\\n",
    "        .withColumn(\"product_id\", col(\"ordered_products\").id) \\\n",
    "        .withColumn(\"unit_price\", col(\"ordered_products\").price) \\\n",
    "        .withColumn(\"quantity\", col(\"ordered_products\").qty)\n",
    "        \n",
    "df1.write.format(\"delta\").saveAsTable(\"retail_org_dt.sales_orders_cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b5f08d-9a32-4e6c-a653-a40bf84dfcb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"dbfs:/user/hive/warehouse/retail_org_dt.db/customers_raw\") \\\n",
    "        .select(\"*\") \\\n",
    "        .withColumn(\"customer_id\",col(\"customer_id\").cast(\"integer\")) \\\n",
    "        .withColumn(\"lon\",col(\"lon\").cast(\"double\")) \\\n",
    "        .withColumn(\"lat\",col(\"lat\").cast(\"double\")) \\\n",
    "        .withColumn(\"loyalty_segment\",col(\"loyalty_segment\").cast(\"integer\"))\n",
    "\n",
    "df2.write.format(\"delta\").saveAsTable(\"retail_org_dt.customers_cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6008254a-7780-4113-940c-5b7e375a5912",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"dbfs:/user/hive/warehouse/retail_org_dt.db/products_raw\") \\\n",
    "        .select(\"*\") \\\n",
    "        .withColumn(\"ean13\",col(\"ean13\").cast(\"double\"))\n",
    "\n",
    "df3.write.format(\"delta\").saveAsTable(\"retail_org_dt.products_cleansed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver-layer-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
