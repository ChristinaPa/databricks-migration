{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45587948-7c5e-4161-82c6-35501ecf86df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0497046-c966-4ce8-b052-ae105960c161",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_schema = \"customer_id integer,tax_id STRING,tax_code STRING, customer_name STRING,state STRING,city STRING,postcode STRING,street STRING,number STRING,unit STRING,region STRING,district STRING,lon double,lat double,ship_to_address STRING,valid_from STRING,valid_to STRING,units_purchased STRING,loyalty_segment integer, customer_key BIGINT\"\n",
    "\n",
    "customers_df = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"dbfs:/user/hive/warehouse/example.db/customers_cleansed_dt\")\n",
    "\n",
    "customers_df = customers_df.withColumn(\"customer_key\", expr(\"monotonically_increasing_id() + 1\"))\n",
    "\n",
    "dim_customers = spark.createDataFrame(customers_df.rdd, customers_schema)\n",
    "\n",
    "dim_customers.write.format(\"delta\").saveAsTable(\"example.dim_customers_dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39b487a-e577-4388-bd36-3b356b15e441",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_schema = \"product_id STRING,product_category STRING,product_name STRING,sales_price STRING,ean13 DOUBLE,ean5 STRING,product_unit STRING, product_key BIGINT\"\n",
    "\n",
    "products_df = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"dbfs:/user/hive/warehouse/example.db/products_cleansed_dt\") \\\n",
    "        .select(\"product_id\", \"product_category\", \"product_name\", \"sales_price\", \"ean13\", \"ean5\", \"product_unit\")\n",
    "\n",
    "products_df = products_df.withColumn(\"product_key\", expr(\"monotonically_increasing_id() + 1\"))\n",
    "\n",
    "dim_products = spark.createDataFrame(products_df.rdd, products_schema)\n",
    "\n",
    "dim_products.write.format(\"delta\").saveAsTable(\"example.dim_products_dt\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold-layer-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
