{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5e9c3f-2825-4a8b-b527-375c89cc2616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f1af1f-8236-4103-9d0b-af1d0fa31150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=\"databricksscopename\",key=\"databricksservicekey\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"5c962712-e111-48b0-b9af-41f03175e216\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adlsx5u224gv3xgd2.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adlsx5u224gv3xgd2.dfs.core.windows.net\", \"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5775a6a6-e9a9-45a1-aa13-924df7c38dbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from azure.eventhub import EventHubProducerClient, EventHubConsumerClient, EventData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180fce99-22e1-4035-aaec-98fde429b07c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_file_path = 'abfss://data@adlsx5u224gv3xgd2.dfs.core.windows.net/sales_orders_stream.json'  # Replace with your JSON file path\n",
    "\n",
    "# Read a json file using pandas\n",
    "df = ps.read_json(json_file_path)\n",
    "df = df.head(5)\n",
    "\n",
    "display(df)\n",
    "# Convert the CSV file into JSON\n",
    "json_data = df.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713eb4a1-82ed-43e7-a21c-0a3beb6fa0f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6925b20d-8340-4239-8a90-89ba96af84ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event_hub_connection_string = 'Endpoint=sb://streamdata-2f774f-ns.servicebus.windows.net/;SharedAccessKeyName=rule;SharedAccessKey={SharedAccessKey}'  # Replace with your connection string\n",
    "event_hub_name = 'test'\n",
    "\n",
    "\n",
    "\n",
    "producer = EventHubProducerClient.from_connection_string(event_hub_connection_string, eventhub_name=event_hub_name)\n",
    "\n",
    "# Publish the JSON as events\n",
    "try:\n",
    "    event_data_batch = producer.create_batch()\n",
    "    event_data_batch.add(EventData(json_data))\n",
    "    producer.send_batch(event_data_batch)\n",
    "except Exception as e:\n",
    "    print('Error occurred: ', str(e))\n",
    "finally:\n",
    "    producer.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "publish_events-eventhub",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
