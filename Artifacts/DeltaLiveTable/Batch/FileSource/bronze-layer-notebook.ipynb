{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read schema from raw data files (Bronze layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b90c5c48-a368-4ca5-a16e-2fae661e3430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt # **dlt** module, which is a Python package for working with Delta Lake\n",
    "from pyspark.sql.functions import * # used for data manipulation and transformation in Spark SQL\n",
    "from pyspark.sql.types import * # provides the data types that can be used to define the schema of a DataFrame or a column in Spark SQL\n",
    "import datetime #imports the datetime module, which is a standard Python library for working with dates and times."
   ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "service_credential = dbutils.secrets.get(scope='scope',key='service-credential-key')\n", 
        "spark.conf.set('fs.azure.account.auth.type.storage-account.dfs.core.windows.net', 'OAuth')\n",
        "spark.conf.set('fs.azure.account.oauth.provider.type.storage-account.dfs.core.windows.net'\n",
        "'org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider')\n",
        "spark.conf.set('fs.azure.account.oauth2.client.id.storage-account.dfs.core.windows.net', 'application-id')\n",
        "spark.conf.set('fs.azure.account.oauth2.client.secret.storage-account.dfs.core.windows.net', service_credential)\n",
        "spark.conf.set('fs.azure.account.oauth2.client.endpoint.storage-account.dfs.core.windows.net'\n", 
        "'https://login.microsoftonline.com/directory-id/oauth2/token')\n",
        "# Replace 'scope' with the Databricks secret scope name.\n",
        "# Replace 'service-credential-key' with the name of the key containing the client secret\n",
        "# Replace 'storage-account' with the name of the Azure storage account.\n",
        "# Replace 'application-id' with the Application (client) ID for the Azure Active Directory application.\n",
        "# Replace 'directory-id' with the Directory (tenant) ID for the Azure Active Directory application\n"
    ]
   },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create delta live table\n",
    "data_path1 ='abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/data1.csv' # represents the file path where the data file is located ## to be changed to actual file name and location\n",
    "df1 = spark.read.csv(data_path1) # read the data from the CSV file located at **data_path1** and load it into a DataFrame named **df1**\n",
    "@dlt.table #represents a decorator\n",
    "def Dim1_raw():\n",
    "    return spark.read.schema(df1.schema).option('header', True).csv(data_path1) # returns a DataFrame by using the **spark.read** object to read the CSV file at **data_path1** infering the schema from **df1** DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create delta live table\n",
    "data_path2 ='abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/data2.csv' # represents the file path where the data file is located ## to be changed to actual file name and location\n",
    "df2 = spark.read.csv(data_path2) # read the data from the CSV file located at **data_path2** and load it into a DataFrame named **df2**\n",
    "@dlt.table #represents a decorator\n",
    "def Dim2_raw():\n",
    "    return spark.read.schema(df2.schema).option('header', True).csv(data_path2) # returns a DataFrame by using the **spark.read** object to read the CSV file at **data_path2** infering the schema from **df2** DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create delta live table\n",
    "data_path3 ='abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/data3.csv' # represents the file path where the data file is located ## to be changed to actual file name and location\n",
    "df3 = spark.read.csv(data_path3) # read the data from the CSV file located at **data_path3** and load it into a DataFrame named **df3**\n",
    "@dlt.table #represents a decorator\n",
    "def Fact_raw():\n",
    "    return spark.read.schema(df3.schema).option('header', True).csv(data_path3) # returns a DataFrame by using the **spark.read** object to read the CSV file at **data_path3** infering the schema from **df3** DataFrame."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RawFiles_bronze-layer-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
