{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read schema from raw data files (Bronze layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b90c5c48-a368-4ca5-a16e-2fae661e3430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt # **dlt** module, which is a Python package for working with Delta Lake\n",
    "from pyspark.sql.functions import * # used for data manipulation and transformation in Spark SQL\n",
    "from pyspark.sql.types import * # provides the data types that can be used to define the schema of a DataFrame or a column in Spark SQL\n",
    "import datetime #imports the datetime module, which is a standard Python library for working with dates and times."
   ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "service_credential = dbutils.secrets.get(scope='<scope>',key='<service-credential-key>')\n", 
        "spark.conf.set('fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net', 'OAuth')\n",
        "spark.conf.set('fs.azure.account.oauth.provider.type.adlsx5u224gv3xgd2.dfs.core.windows.net','org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider')\n",
        "spark.conf.set('fs.azure.account.oauth2.client.id.adlsx5u224gv3xgd2.dfs.core.windows.net', '<application-id>')\n",
        "spark.conf.set('fs.azure.account.oauth2.client.secret.adlsx5u224gv3xgd2.dfs.core.windows.net', service_credential)\n",
        "spark.conf.set('fs.azure.account.oauth2.client.endpoint.adlsx5u224gv3xgd2.dfs.core.windows.net', 'https://login.microsoftonline.com/<directory-id>/oauth2/token')\n",
        "#Replace <scope> with the Databricks secret scope name.\n",
        "#Replace <service-credential-key> with the name of the key containing the client secret\n",
        "#Replace <storage-account> with the name of the Azure storage account.\n",
        "#Replace <application-id> with the Application (client) ID for the Azure Active Directory application.\n",
        "#Replace <directory-id> with the Directory (tenant) ID for the Azure Active Directory application\n"
    ]
   },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create delta live table\n",
    "data_path1 ='/mnt/data/data1.csv' # represents the file path where the data file is located ## to be changed to actual file name\n",
    "df1 = spark.read.csv(data_path1) # read the data from the CSV file located at **data_path1** and load it into a DataFrame named **df1**\n",
    "@dlt.table #represents a decorator\n",
    "def Dim1_raw():\n",
    "    return spark.read.schema(df1.schema).option('header', True).csv(data_path1) # returns a DataFrame by using the **spark.read** object to read the CSV file at **data_path1** infering the schema from **df1** DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create delta live table\n",
    "data_path2 ='/mnt/data/data2.csv' # represents the file path where the data file is located ## to be changed to actual file name\n",
    "df2 = spark.read.csv(data_path2) # read the data from the CSV file located at **data_path2** and load it into a DataFrame named **df2**\n",
    "@dlt.table #represents a decorator\n",
    "def Dim2_raw():\n",
    "    return spark.read.schema(df2.schema).option('header', True).csv(data_path2) # returns a DataFrame by using the **spark.read** object to read the CSV file at **data_path2** infering the schema from **df2** DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create delta live table\n",
    "data_path3 ='/mnt/data/data3.csv' # represents the file path where the data file is located ## to be changed to actual file name\n",
    "df3 = spark.read.csv(data_path3) # read the data from the CSV file located at **data_path3** and load it into a DataFrame named **df3**\n",
    "@dlt.table #represents a decorator\n",
    "def Fact_raw():\n",
    "    return spark.read.schema(df3.schema).option('header', True).csv(data_path3) # returns a DataFrame by using the **spark.read** object to read the CSV file at **data_path3** infering the schema from **df3** DataFrame."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RawFiles_bronze-layer-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
