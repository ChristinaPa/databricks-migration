{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee820cb9-fd40-4cb1-8bd2-61d7323f5682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "Topic_Name1 = {topic1}\n",
    "Topic_Name2 = {topic2}\n",
    "Topic_Name3 = {topic3}\n",
    "EH_NS_NAME = {Event Hb Name}\n",
    "BOOTSTRAP_SERVERS = f\"{EH_NS_NAME}.servicebus.windows.net:9093\"\n",
    "SAKEY = \"UR+tdi5brOqFxphEl2rZdwszylRHA3tkwhOqsdqA464=\"\n",
    "CONN_STRING = f\"Endpoint=sb://{EH_NS_NAME}.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey={SAKEY}\"\n",
    "LOGIN_MODULE = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule\"\n",
    "EH_SASL = (\n",
    "    f'{LOGIN_MODULE} required username=\"$ConnectionString\" password=\"{CONN_STRING}\";'\n",
    ")\n",
    "\n",
    "# --Create first bronze table--#\n",
    "\n",
    "df1 = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\")\n",
    "    .option(\"subscribe\", Topic_Name1)  \n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL)\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "df1 = df.withColumn(\n",
    "    \"key\", col(\"key\").cast(\"string\")\n",
    ").withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "\n",
    "# Create raw delta live table\n",
    "@dlt.table(\n",
    "    table_properties={\"pipelines.reset.allowed\": \"false\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"1 seconds\"},\n",
    ")\n",
    "def data1_raw():\n",
    "    return df1\n",
    "\n",
    "# --Create second bronze table--#\n",
    "df2 = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\")\n",
    "    .option(\"subscribe\", Topic_Name2)  \n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL)\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "df2 = df2.withColumn(\"key\", col(\"key\").cast(\"string\")).withColumn(\n",
    "    \"value\", col(\"value\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Create raw delta live table\n",
    "@dlt.table(\n",
    "    #     table_properties={\"pipelines.reset.allowed\": \"false\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"1 seconds\"},\n",
    ")\n",
    "def data2_raw():\n",
    "    return df2\n",
    "\n",
    "# --Create third bronze table--#\n",
    "df3 = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{EH_NS_NAME}.servicebus.windows.net:9093\")\n",
    "    .option(\"subscribe\", Topic_Name3)  \n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL)\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert from base64 to string\n",
    "df3 = df3.withColumn(\"key\", col(\"key\").cast(\"string\")).withColumn(\n",
    "    \"value\", col(\"value\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Create raw delta live table\n",
    "@dlt.table(\n",
    "    table_properties={\"pipelines.reset.allowed\": \"false\"},\n",
    "    spark_conf={\"pipelines.trigger.interval\": \"1 seconds\"},\n",
    ")\n",
    "def data3_raw():\n",
    "    return df3"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "eventhub_bronze-layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
